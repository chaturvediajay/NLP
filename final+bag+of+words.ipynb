{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "final+bag+of+words.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chaturvediajay/NLP/blob/main/final%2Bbag%2Bof%2Bwords.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HOhbVHh2cBhX"
      },
      "source": [
        "### Bag of words model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Kn0tFW5cClK"
      },
      "source": [
        ""
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jFRPBmpkcHHL",
        "outputId": "fccd7961-df8e-424b-c322-941562dd1338"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MgfvLBzgcBhc",
        "outputId": "c264dd68-95f6-4cde-fb23-a25fc8e6a9bc"
      },
      "source": [
        "# load all necessary libraries\n",
        "import pandas as pd\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "pd.set_option('max_colwidth', 100)\n",
        "\n",
        "import nltk\n",
        "words=nltk.download('punkt')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pxw-WzRucBhf"
      },
      "source": [
        "#### Let's build a basic bag of words model on three sample documents"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ePKuk4NccBhg",
        "outputId": "19fad22f-a866-46ec-ab29-2e70588f3ff2"
      },
      "source": [
        "documents = [\"Gangs of Wasseypur is a great movie.\", \"The success of a movie depends on the performance of the actors.\", \"There are no new movies releasing this week.\"]\n",
        "print(documents)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Gangs of Wasseypur is a great movie.', 'The success of a movie depends on the performance of the actors.', 'There are no new movies releasing this week.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 358
        },
        "id": "7tXNmsijcBhi",
        "outputId": "f3c9c959-e3f6-4604-8eb2-8166bbec7504"
      },
      "source": [
        "def preprocess(document):\n",
        "    'changes document to lower case and removes stopwords'\n",
        "\n",
        "    # change sentence to lower case\n",
        "    document = document.lower()\n",
        "\n",
        "    # tokenize into words\n",
        "    # words = word_tokenize(document)\n",
        "\n",
        "\n",
        "    # remove stop words\n",
        "    words = [word for word in words if word not in stopwords.words(\"english\")]\n",
        "\n",
        "    # join words to make sentence\n",
        "    document = \" \".join(words)\n",
        "    \n",
        "    return document\n",
        "\n",
        "documents = [preprocess(document) for document in documents]\n",
        "print(documents)\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "error",
          "ename": "UnboundLocalError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-a65388e17a45>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdocument\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mdocuments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdocument\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocuments\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-a65388e17a45>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdocument\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mdocuments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdocument\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocuments\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-a65388e17a45>\u001b[0m in \u001b[0;36mpreprocess\u001b[0;34m(document)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# remove stop words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"english\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# join words to make sentence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'words' referenced before assignment"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3jD0bHSDcBhj"
      },
      "source": [
        "#### Creating bag of words model using count vectorizer function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2kOZe-ObcBhl"
      },
      "source": [
        "vectorizer = CountVectorizer()\n",
        "bow_model = vectorizer.fit_transform(documents)\n",
        "print(bow_model)  # returns the row number and column number of the cells which have 1 as value"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2yHbEWz-cBhm"
      },
      "source": [
        "# print the full sparse matrix\n",
        "print(bow_model.toarray())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aGO65z1ycBhn"
      },
      "source": [
        "print(bow_model.shape)\n",
        "print(vectorizer.get_feature_names())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4BUiZYvUcBho"
      },
      "source": [
        "### Let's create a bag of words model on the spam dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AY9eSlj9cBhp"
      },
      "source": [
        "# load data\n",
        "spam = pd.read_csv(\"/content/drive/MyDrive/al_ml_project/NLP/SMSSpamCollection.txt\", sep = \"\\t\", names=[\"label\", \"message\"])\n",
        "spam.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dNnlgGD4cBhq"
      },
      "source": [
        "##### Let's take a subset of data (first 50 rows only) and create bag of word model on that."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "43THQd9ycBhq"
      },
      "source": [
        "spam = spam.iloc[0:50,:]\n",
        "print(spam)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D8k-BrP_cBhr"
      },
      "source": [
        "# extract the messages from the dataframe\n",
        "messages = spam.message\n",
        "print(messages)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IpkShLmncBhs"
      },
      "source": [
        "# convert messages into list\n",
        "messages = [message for message in messages]\n",
        "print(messages)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z2YhsvqAcBhs"
      },
      "source": [
        "# preprocess messages using the preprocess function\n",
        "messages = [preprocess(message) for message in messages]\n",
        "print(messages)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6e_YmwVRcBht"
      },
      "source": [
        "# bag of words model\n",
        "vectorizer = CountVectorizer()\n",
        "bow_model = vectorizer.fit_transform(messages)\n",
        "print(bow_model.toarray())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "SlGsZ4UOcBht"
      },
      "source": [
        "print(bow_model.shape)\n",
        "print(vectorizer.get_feature_names())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aa4boqLocBhu"
      },
      "source": [
        "* A lot of duplicate tokens such as 'win'and 'winner'; 'reply' and 'replying'; 'want' and 'wanted' etc. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-jy6GXscBhu"
      },
      "source": [
        "## Stemming and lemmatising"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "scrolled": true,
        "id": "OSMjerzScBhu"
      },
      "source": [
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# add stemming and lemmatisation in the preprocess function\n",
        "def preprocess(document, stem=True):\n",
        "    'changes document to lower case and removes stopwords'\n",
        "\n",
        "    # change sentence to lower case\n",
        "    document = document.lower()\n",
        "\n",
        "    # tokenize into words\n",
        "    words = word_tokenize(document)\n",
        "\n",
        "    # remove stop words\n",
        "    words = [word for word in words if word not in stopwords.words(\"english\")]\n",
        "    \n",
        "    if stem:\n",
        "        words = [stemmer.stem(word) for word in words]\n",
        "    else:\n",
        "        words = [wordnet_lemmatizer.lemmatize(word, pos='v') for word in words]\n",
        "\n",
        "    # join words to make sentence\n",
        "    document = \" \".join(words)\n",
        "    \n",
        "    return document"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5oRJrKljcBhv"
      },
      "source": [
        "### Bag of words model on stemmed messages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "scrolled": true,
        "id": "MmH0RpzDcBhv"
      },
      "source": [
        "# stem messages\n",
        "messages = [preprocess(message, stem=True) for message in spam.message]\n",
        "\n",
        "# bag of words model\n",
        "vectorizer = CountVectorizer()\n",
        "bow_model = vectorizer.fit_transform(messages)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "Gh7SZqS8cBhw"
      },
      "source": [
        "# look at the dataframe\n",
        "pd.DataFrame(bow_model.toarray(), columns = vectorizer.get_feature_names())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MT0I6wX0cBhw"
      },
      "source": [
        "# token names\n",
        "print(vectorizer.get_feature_names())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98JD6Z1dcBhx"
      },
      "source": [
        "### 359 tokens after stemming the messages as compared to 381 tokens without stemming.\n",
        "\n",
        "### Let's try lemmatizing the messages."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "scrolled": true,
        "id": "Q7DHEz0PcBhx"
      },
      "source": [
        "# lemmatise messages\n",
        "messages = [preprocess(message, stem=False) for message in spam.message]\n",
        "\n",
        "# bag of words model\n",
        "vectorizer = CountVectorizer()\n",
        "bow_model = vectorizer.fit_transform(messages)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "yCuKzcGScBhx"
      },
      "source": [
        "# look at the dataframe\n",
        "pd.DataFrame(bow_model.toarray(), columns = vectorizer.get_feature_names())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xi8fdNLWcBhx"
      },
      "source": [
        "# token names\n",
        "print(vectorizer.get_feature_names())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rEUAGwxjcBhy"
      },
      "source": [
        "### 363 tokens after lemmatizing the messages as compared to 381 tokens without lemmatising. But, on the other hand, stemmer reduces the token count to 359. Lemmatization doesn't work as expected because the data is very unclean."
      ]
    }
  ]
}